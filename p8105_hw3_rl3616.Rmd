---
title: "p8105_hw3_rl3616"
author: "Ruipeng Li"
date: "`r format(Sys.Date())`"
output: github_document
---

# Problem 1

Loading data
```{r}
library(p8105.datasets)
library(ggplot2)
library(tidyverse)
data("instacart") 
```

#### Part a)
The Instacart dataset contains 1,384,617 observations and 15 variables, representing individual items from customersâ€™ grocery orders.

Each observation corresponds to one product within an order, with variables describing the order time `order_hour_of_day`, date in week `order_dow`, customer `user_id`, product details `product_name, aisle, department`, and purchase behavior `add_to_cart_order, reordered, days_since_prior_order`.

The dataset allows exploration of shopping habits such as the most popular aisles, frequently reordered products, and peak shopping hours.

#### Part b)
Also, there are `r n_distinct(instacart$aisle)` different aisles in the dataset, and 
```{r}
top_aisles <- instacart |> 
  count(aisle, sort = TRUE)
```

With this data, we can see the top 1 aisle that most items ordered from was "`r top_aisles$aisle[1]`" and follow up with "`r top_aisles$aisle[2]`" and "`r top_aisles$aisle[3]`"

#### Part c)
```{r}
aisle_plot <- top_aisles |> 
  filter(n > 10000) |> 
  ggplot(aes(x = reorder(aisle, n), y = n)) +
  geom_col(width = 0.7, fill = "steelblue") +
  geom_text(aes(label = scales::comma(n)), 
            hjust = -0.1, size = 2.5) +
  coord_flip() +
  labs(
    title = "Aisles with more than 10,000 items ordered",
       x = "Aisle", y = "Number of items ordered"
    ) +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal(base_size = 8)

ggsave("figures/Problem 1/aisle_plot.png", aisle_plot, width = 10, dpi = 300)
```

#### Part d)
```{r}
top_item_table <- instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle, product_name) |>
  summarise(times_ordered = n()) |> 
  slice_max(times_ordered, n = 3)
```

#### Part e)
```{r}
mean_hour_table <- instacart |> 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |> 
  group_by(product_name, order_dow) |> 
  summarise(mean_hour = mean(order_hour_of_day)) |> 
  mutate(order_dow = factor(order_dow,
    levels = 0:6,
    labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
  )) |>
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

# Problem 2

#### Cleaning Zillow datasets(from homework 2)
```{r}

zip_zori_df <- read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |>
  rename(zip_code = region_name) |> 
  select(-region_type, -state_name, -state, -city, -metro) |> 
  pivot_longer(
    cols = starts_with("x20"),
    names_to = "date",
    values_to = "rent_price"
  ) |> 
  filter(!is.na(rent_price)) |>
  mutate(
    county_name = str_replace(county_name, " County", ""),
    date = str_replace(date, "x", ""),
    date = ymd(date),
    borough = case_when(
      county_name == "New York" ~ "Manhattan",
      county_name == "Kings" ~ "Brooklyn",
      county_name == "Queens" ~ "Queens",
      county_name == "Bronx" ~ "Bronx",
      county_name == "Richmond" ~ "Staten Island")
    ) |> 
   relocate("county_name","borough")

```

```{r}

zip_count <- zip_zori_df |> 
  group_by(zip_code, county_name) |> 
  summarise(n_observed = n()) |> 
  arrange(desc(n_observed))

```
Using this data, we can see there are `r sum(zip_count$n_observed == 116)` ZIP codes are observed 116 times, and there are `r sum(zip_count$n_observed < 10)` ZIP codes observed fewer than 10 times. 

Using arranged data we can see almost all ZIPs in Manhattan and Brooklyn appear 116 times;
while many in Staten Island or the Bronx might appear only a dozen or fewer times. I think it's really depends on population density. Which frequent appearances indicate rich data and active listings; few appearances indicate an unpopular area, a change in listings, or missing data.

#### Average rental price
```{r}
mean_price <- zip_zori_df |> 
  mutate(year = lubridate::year(date)) |>
  group_by(year, borough) |> 
  summarise(average_price = mean(rent_price)) |> 
  pivot_wider(
    names_from = year,
    values_from = average_price
  )
```
